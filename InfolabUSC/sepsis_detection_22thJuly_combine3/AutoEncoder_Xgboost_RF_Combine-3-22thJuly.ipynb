{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/usr/local/lib/python2.7/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n",
      "/usr/local/lib/python2.7/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from itertools import chain\n",
    "\n",
    "# import nltk\n",
    "import sklearn\n",
    "import scipy.stats\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.grid_search import RandomizedSearchCV\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from utils_2 import *\n",
    "import pickle\n",
    "import time\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras.callbacks import Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Finished ', 0)\n",
      "('Finished ', 500)\n",
      "('Finished ', 1000)\n",
      "('Finished ', 1500)\n",
      "('Finished ', 2000)\n",
      "('Finished ', 2500)\n",
      "('Finished ', 3000)\n",
      "('Finished ', 3500)\n",
      "('Finished ', 4000)\n",
      "('Finished ', 4500)\n",
      "('Finished ', 5000)\n",
      "('Finished ', 5500)\n",
      "('Finished ', 6000)\n",
      "('Finished ', 6500)\n",
      "('Finished ', 7000)\n",
      "('Finished ', 7500)\n",
      "('Finished ', 8000)\n",
      "('Finished ', 8500)\n",
      "('Finished ', 9000)\n",
      "('Finished ', 9500)\n",
      "('Finished ', 10000)\n",
      "('Finished ', 10500)\n",
      "('Finished ', 11000)\n",
      "('Finished ', 11500)\n",
      "('Finished ', 12000)\n",
      "('Finished ', 12500)\n",
      "('Finished ', 13000)\n",
      "('Finished ', 13500)\n",
      "('Finished ', 14000)\n",
      "('Finished ', 14500)\n",
      "('Finished ', 15000)\n",
      "('Finished ', 15500)\n",
      "('Finished ', 16000)\n",
      "('Finished ', 16500)\n",
      "('Finished ', 17000)\n",
      "('Finished ', 17500)\n",
      "('Finished ', 18000)\n",
      "('Finished ', 18500)\n",
      "('Finished ', 19000)\n",
      "('Finished ', 19500)\n",
      "('Finished ', 20000)\n",
      "('Finished ', 20500)\n",
      "('Finished ', 21000)\n",
      "('Finished ', 21500)\n",
      "('Finished ', 22000)\n",
      "('Finished ', 22500)\n",
      "('Finished ', 23000)\n",
      "('Finished ', 23500)\n",
      "('Finished ', 24000)\n",
      "('Finished ', 24500)\n",
      "('Finished ', 25000)\n",
      "('Finished ', 25500)\n",
      "('Finished ', 26000)\n",
      "('Finished ', 26500)\n",
      "('Finished ', 27000)\n",
      "('Finished ', 27500)\n",
      "('Finished ', 28000)\n",
      "('Finished ', 28500)\n",
      "('Finished ', 29000)\n",
      "('Finished ', 29500)\n",
      "('Finished ', 30000)\n",
      "('Finished ', 30500)\n",
      "('Finished ', 31000)\n",
      "('Finished ', 31500)\n",
      "('Finished ', 32000)\n",
      "('Finished ', 32500)\n",
      "('Finished ', 33000)\n",
      "('Finished ', 33500)\n",
      "('Finished ', 34000)\n",
      "('Finished ', 34500)\n",
      "('Finished ', 35000)\n",
      "('Finished ', 35500)\n",
      "('Finished ', 36000)\n",
      "('Finished ', 36500)\n",
      "('Finished ', 37000)\n",
      "('Finished ', 37500)\n",
      "('Finished ', 38000)\n",
      "('Finished ', 38500)\n",
      "('Finished ', 39000)\n",
      "('Finished ', 39500)\n",
      "('Finished ', 40000)\n",
      "HR                   83.805115\n",
      "O2Sat                97.153822\n",
      "Temp                 36.842866\n",
      "SBP                 123.321435\n",
      "MAP                  82.563081\n",
      "DBP                  64.095876\n",
      "Resp                 18.537966\n",
      "EtCO2                33.090961\n",
      "BaseExcess           -0.471225\n",
      "HCO3                 24.384593\n",
      "FiO2                  0.577384\n",
      "pH                    7.382065\n",
      "PaCO2                40.896458\n",
      "SaO2                 93.671257\n",
      "AST                 136.705351\n",
      "BUN                  22.024977\n",
      "Alkalinephos         96.849008\n",
      "Calcium               8.029118\n",
      "Chloride            105.550810\n",
      "Creatinine            1.432324\n",
      "Bilirubin_direct      1.266791\n",
      "Glucose             131.810975\n",
      "Lactate               2.162096\n",
      "Magnesium             2.021546\n",
      "Phosphate             3.542320\n",
      "Potassium             4.096041\n",
      "Bilirubin_total       1.459475\n",
      "TroponinI             5.374386\n",
      "Hct                  32.044117\n",
      "Hgb                  10.717368\n",
      "PTT                  37.208017\n",
      "WBC                  11.070723\n",
      "Fibrinogen          304.357275\n",
      "Platelets           206.774898\n",
      "Age                  61.643423\n",
      "Gender                0.559451\n",
      "Unit1                 0.496258\n",
      "Unit2                 0.503742\n",
      "HospAdmTime         -51.848945\n",
      "ICULOS               20.269114\n",
      "SepsisLabel           0.028788\n",
      "dtype: float64\n",
      "('finished imputing, ', 0)\n",
      "('finished imputing, ', 1000)\n",
      "('finished imputing, ', 2000)\n",
      "('finished imputing, ', 3000)\n",
      "('finished imputing, ', 4000)\n",
      "('finished imputing, ', 5000)\n",
      "('finished imputing, ', 6000)\n",
      "('finished imputing, ', 7000)\n",
      "('finished imputing, ', 8000)\n",
      "('finished imputing, ', 9000)\n",
      "('finished imputing, ', 10000)\n",
      "('finished imputing, ', 11000)\n",
      "('finished imputing, ', 12000)\n",
      "('finished imputing, ', 13000)\n",
      "('finished imputing, ', 14000)\n",
      "('finished imputing, ', 15000)\n",
      "('finished imputing, ', 16000)\n",
      "('finished imputing, ', 17000)\n",
      "('finished imputing, ', 18000)\n",
      "('finished imputing, ', 19000)\n",
      "('finished imputing, ', 20000)\n",
      "('finished imputing, ', 21000)\n",
      "('finished imputing, ', 22000)\n",
      "('finished imputing, ', 23000)\n",
      "('finished imputing, ', 24000)\n",
      "('finished imputing, ', 25000)\n",
      "('finished imputing, ', 26000)\n",
      "('finished imputing, ', 27000)\n",
      "('finished imputing, ', 28000)\n",
      "('finished imputing, ', 29000)\n",
      "('finished imputing, ', 30000)\n",
      "('finished imputing, ', 31000)\n",
      "('finished imputing, ', 32000)\n",
      "('finished imputing, ', 33000)\n",
      "('finished imputing, ', 34000)\n",
      "('finished imputing, ', 35000)\n",
      "('finished imputing, ', 36000)\n",
      "('finished imputing, ', 37000)\n",
      "('finished imputing, ', 38000)\n",
      "('finished imputing, ', 39000)\n",
      "('finished imputing, ', 40000)\n"
     ]
    }
   ],
   "source": [
    "all_data, all_files = read_training_data_raw(training_path = '/home/tranvanluan7/sepsis_detection/new_training/')\n",
    "mean_features = [data.mean(axis=0, skipna=True) for data in all_data]\n",
    "concate_features = pd.concat(mean_features, axis=1)\n",
    "mean_features = concate_features.mean(axis=1, skipna=True)\n",
    "print(mean_features)\n",
    "\n",
    "\n",
    "# impute missing data\n",
    "def impute_missing_data(df):\n",
    "    #df = df.interpolate(method ='linear')\n",
    "    df2 = df.copy()\n",
    "    for column_name in df2.columns:\n",
    "        replace_value = mean_features[column_name]\n",
    "        df2[column_name] = df2[column_name].fillna(value=replace_value)\n",
    "    return df2\n",
    "\n",
    "imputed_data = []\n",
    "for idx,data in enumerate(all_data):\n",
    "    imputed_data.append(impute_missing_data(data))\n",
    "    if idx % 1000 == 0:\n",
    "        print(\"finished imputing, \",idx)\n",
    "        \n",
    "all_data_imputed_values = [x.loc[:,x.columns!='SepsisLabel'] for x in imputed_data]\n",
    "all_data_labels = [x.loc[:, x.columns=='SepsisLabel'] for x in all_data]\n",
    "all_data_raw_values =  [x.loc[:,x.columns!='SepsisLabel'] for x in all_data]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separate into train,  test data\n",
    "X_imputed_train, X_imputed_test, y_imputed_train, y_imputed_test, X_raw_train, X_raw_test = train_test_split(\n",
    "    all_data_imputed_values, all_data_labels,\n",
    "    all_data_raw_values, \n",
    "    test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing features for net \n",
      "('Finished computing features', 0)\n",
      "('Finished computing features', 1000)\n",
      "('Finished computing features', 2000)\n",
      "('Finished computing features', 3000)\n",
      "('Finished computing features', 4000)\n",
      "('Finished computing features', 5000)\n",
      "('Finished computing features', 6000)\n",
      "('Finished computing features', 7000)\n",
      "('Finished computing features', 8000)\n",
      "('Finished computing features', 9000)\n",
      "('Finished computing features', 10000)\n",
      "('Finished computing features', 11000)\n",
      "('Finished computing features', 12000)\n",
      "('Finished computing features', 13000)\n",
      "('Finished computing features', 14000)\n",
      "('Finished computing features', 15000)\n",
      "('Finished computing features', 16000)\n",
      "('Finished computing features', 17000)\n",
      "('Finished computing features', 18000)\n",
      "('Finished computing features', 19000)\n",
      "('Finished computing features', 20000)\n",
      "('Finished computing features', 21000)\n",
      "('Finished computing features', 22000)\n",
      "('Finished computing features', 23000)\n",
      "('Finished computing features', 24000)\n",
      "('Finished computing features', 25000)\n",
      "('Finished computing features', 26000)\n",
      "('Finished computing features', 27000)\n",
      "('Train feature shape', (49542, 240))\n",
      "('After normalized, shape = ', (49542, 6, 40))\n"
     ]
    }
   ],
   "source": [
    "#Compute features\n",
    "print(\"Computing features for net \")\n",
    "nr_imputed_features = []\n",
    "nr_labels = []\n",
    "train_labels = []\n",
    "for idx, data in enumerate(X_imputed_train):\n",
    "    num_row = data.shape[0]\n",
    "    num_col = data.shape[1]\n",
    "    \n",
    "    for i in range(5, num_row-1):\n",
    "        d = data.loc[(i-5):i].values #Select recent 6 hours data as features \n",
    "        \n",
    "        missing_percent = X_raw_train[idx].loc[(i-5):i].isna().mean().round(4).sum()\n",
    "        \n",
    "        l = int(y_imputed_train[idx].loc[i])\n",
    "        if np.random.randint(0,100) > 95 or l == 1: #Select only 25% label 0\n",
    "            nr_imputed_features.append(d)\n",
    "            train_labels.append(l)\n",
    "            if l == 1:\n",
    "                nr_labels.append([0,1])\n",
    "            else:\n",
    "                nr_labels.append([1,0])\n",
    "    if idx % 1000 == 0:\n",
    "        print(\"Finished computing features\", idx)\n",
    "\n",
    "#Reshap train features for normalization\n",
    "train_features = np.array(nr_imputed_features)\n",
    "train_features = train_features.reshape((-1, 240))\n",
    "print(\"Train feature shape\", train_features.shape)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "normalized_train_features = scaler.fit_transform(train_features)\n",
    "normalized_train_features = normalized_train_features.reshape((-1, 6, 40))\n",
    "\n",
    "print(\"After normalized, shape = \", normalized_train_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import *\n",
    "from keras.models import Model\n",
    "# LSTM Autoencoder model\n",
    "# this is the size of our encoded representations\n",
    "encoding_dim = 64  \n",
    "\n",
    "# this is our input placeholder\n",
    "input_img = Input(shape=(6, 40))\n",
    "x = BatchNormalization()(input_img)\n",
    "encoded = LSTM(128, activation='relu', return_sequences=True)(x)\n",
    "encoded = LSTM(64, activation='relu', return_sequences=True)(encoded)\n",
    "encoded = LSTM(encoding_dim, activation='relu')(encoded)\n",
    "\n",
    "decoded = RepeatVector(6)(encoded)\n",
    "decoded = LSTM(64, activation='relu', return_sequences=True)(decoded)\n",
    "decoded = LSTM(128, activation='relu', return_sequences=True)(decoded)\n",
    "decoded = TimeDistributed(Dense(40, activation='sigmoid'))(decoded)\n",
    "\n",
    "classification_out = Dense(32, activation='relu')(encoded)\n",
    "classification_out = Dropout(0.3)(classification_out)\n",
    "classification_out = Dense(24, activation='relu')(classification_out)\n",
    "classification_out = Dropout(0.3)(classification_out)\n",
    "classification_out = Dense(16, activation='relu')(classification_out)\n",
    "classification_out = Dropout(0.3)(classification_out)\n",
    "classification_out = Dense(8, activation='relu')(classification_out)\n",
    "classification_out = Dense(2, activation='softmax')(classification_out)\n",
    "# this model maps an input to its reconstruction\n",
    "autoencoder = Model(input_img, [decoded, classification_out])\n",
    "encoder = Model(input_img, encoded)\n",
    "classifier = Model(input_img, classification_out)\n",
    "\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "from keras.optimizers import SGD\n",
    "adam = Adam(lr=0.001)\n",
    "autoencoder.compile(optimizer=adam, loss=['mean_squared_error','categorical_crossentropy'],\n",
    "                    loss_weights=[0.2, 0.8])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 39633 samples, validate on 9909 samples\n",
      "Epoch 1/100\n",
      "39633/39633 [==============================] - 36s 903us/step - loss: 0.4641 - time_distributed_1_loss: 0.0486 - dense_6_loss: 0.5680 - val_loss: 0.4197 - val_time_distributed_1_loss: 0.0171 - val_dense_6_loss: 0.5203\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.41966, saving model to best_model_autoencoder_Manh_RF\n",
      "('lr:', 0.000970873786407767)\n",
      "Epoch 2/100\n",
      "39633/39633 [==============================] - 10s 256us/step - loss: 0.4042 - time_distributed_1_loss: 0.0159 - dense_6_loss: 0.5012 - val_loss: 0.3930 - val_time_distributed_1_loss: 0.0155 - val_dense_6_loss: 0.4873\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.41966 to 0.39298, saving model to best_model_autoencoder_Manh_RF\n",
      "('lr:', 0.0009615384615384615)\n",
      "Epoch 3/100\n",
      "39633/39633 [==============================] - 11s 267us/step - loss: 0.3804 - time_distributed_1_loss: 0.0137 - dense_6_loss: 0.4721 - val_loss: 0.3876 - val_time_distributed_1_loss: 0.0124 - val_dense_6_loss: 0.4814\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.39298 to 0.38759, saving model to best_model_autoencoder_Manh_RF\n",
      "('lr:', 0.0009523809523809524)\n",
      "Epoch 4/100\n",
      "39633/39633 [==============================] - 10s 249us/step - loss: 0.3651 - time_distributed_1_loss: 0.0115 - dense_6_loss: 0.4535 - val_loss: 0.3990 - val_time_distributed_1_loss: 0.0103 - val_dense_6_loss: 0.4961\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.38759\n",
      "('lr:', 0.0009433962264150943)\n",
      "Epoch 5/100\n",
      "39633/39633 [==============================] - 10s 253us/step - loss: 0.3525 - time_distributed_1_loss: 0.0089 - dense_6_loss: 0.4384 - val_loss: 0.3958 - val_time_distributed_1_loss: 0.0075 - val_dense_6_loss: 0.4929\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.38759\n",
      "('lr:', 0.0009345794392523364)\n",
      "Epoch 6/100\n",
      "39633/39633 [==============================] - 10s 255us/step - loss: 0.3400 - time_distributed_1_loss: 0.0068 - dense_6_loss: 0.4233 - val_loss: 0.4336 - val_time_distributed_1_loss: 0.0064 - val_dense_6_loss: 0.5403\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.38759\n",
      "('lr:', 0.0009259259259259259)\n",
      "Epoch 7/100\n",
      "39633/39633 [==============================] - 10s 257us/step - loss: 0.3250 - time_distributed_1_loss: 0.0057 - dense_6_loss: 0.4048 - val_loss: 0.4358 - val_time_distributed_1_loss: 0.0054 - val_dense_6_loss: 0.5433\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.38759\n",
      "('lr:', 0.0009174311926605504)\n",
      "Epoch 8/100\n",
      "39633/39633 [==============================] - 10s 259us/step - loss: 0.3107 - time_distributed_1_loss: 0.0049 - dense_6_loss: 0.3871 - val_loss: 0.4441 - val_time_distributed_1_loss: 0.0049 - val_dense_6_loss: 0.5539\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.38759\n",
      "('lr:', 0.0009090909090909091)\n",
      "Epoch 9/100\n",
      "39633/39633 [==============================] - 10s 255us/step - loss: 0.2990 - time_distributed_1_loss: 0.0047 - dense_6_loss: 0.3725 - val_loss: 0.4759 - val_time_distributed_1_loss: 0.0046 - val_dense_6_loss: 0.5937\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.38759\n",
      "('lr:', 0.0009009009009009008)\n",
      "Epoch 10/100\n",
      "39633/39633 [==============================] - 10s 258us/step - loss: 0.2838 - time_distributed_1_loss: 0.0043 - dense_6_loss: 0.3536 - val_loss: 0.4903 - val_time_distributed_1_loss: 0.0044 - val_dense_6_loss: 0.6118\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.38759\n",
      "('lr:', 0.0008928571428571428)\n",
      "Epoch 11/100\n",
      "39633/39633 [==============================] - 10s 255us/step - loss: 0.2697 - time_distributed_1_loss: 0.0041 - dense_6_loss: 0.3361 - val_loss: 0.4687 - val_time_distributed_1_loss: 0.0040 - val_dense_6_loss: 0.5848\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.38759\n",
      "('lr:', 0.0008849557522123895)\n",
      "Epoch 12/100\n",
      "39633/39633 [==============================] - 10s 254us/step - loss: 0.2536 - time_distributed_1_loss: 0.0038 - dense_6_loss: 0.3160 - val_loss: 0.5677 - val_time_distributed_1_loss: 0.0040 - val_dense_6_loss: 0.7086\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.38759\n",
      "('lr:', 0.0008771929824561403)\n",
      "Epoch 13/100\n",
      "39633/39633 [==============================] - 10s 251us/step - loss: 0.2457 - time_distributed_1_loss: 0.0038 - dense_6_loss: 0.3062 - val_loss: 0.5283 - val_time_distributed_1_loss: 0.0042 - val_dense_6_loss: 0.6593\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.38759\n",
      "('lr:', 0.0008695652173913044)\n",
      "Epoch 14/100\n",
      "39633/39633 [==============================] - 10s 258us/step - loss: 0.2295 - time_distributed_1_loss: 0.0038 - dense_6_loss: 0.2859 - val_loss: 0.6476 - val_time_distributed_1_loss: 0.0039 - val_dense_6_loss: 0.8085\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.38759\n",
      "('lr:', 0.0008620689655172415)\n",
      "Epoch 15/100\n",
      "39633/39633 [==============================] - 11s 265us/step - loss: 0.2214 - time_distributed_1_loss: 0.0038 - dense_6_loss: 0.2758 - val_loss: 0.6221 - val_time_distributed_1_loss: 0.0040 - val_dense_6_loss: 0.7766\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.38759\n",
      "('lr:', 0.0008547008547008548)\n",
      "Epoch 16/100\n",
      "39633/39633 [==============================] - 10s 259us/step - loss: 0.2093 - time_distributed_1_loss: 0.0039 - dense_6_loss: 0.2606 - val_loss: 0.6765 - val_time_distributed_1_loss: 0.0039 - val_dense_6_loss: 0.8446\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.38759\n",
      "('lr:', 0.0008474576271186442)\n",
      "Epoch 17/100\n",
      "39633/39633 [==============================] - 10s 261us/step - loss: 0.1917 - time_distributed_1_loss: 0.0039 - dense_6_loss: 0.2386 - val_loss: 0.6918 - val_time_distributed_1_loss: 0.0038 - val_dense_6_loss: 0.8638\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.38759\n",
      "('lr:', 0.0008403361344537816)\n",
      "Epoch 18/100\n",
      "39633/39633 [==============================] - 10s 256us/step - loss: 0.1823 - time_distributed_1_loss: 0.0038 - dense_6_loss: 0.2269 - val_loss: 0.7164 - val_time_distributed_1_loss: 0.0039 - val_dense_6_loss: 0.8945\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.38759\n",
      "('lr:', 0.0008333333333333334)\n",
      "Epoch 19/100\n",
      "39633/39633 [==============================] - 10s 258us/step - loss: 0.1776 - time_distributed_1_loss: 0.0039 - dense_6_loss: 0.2211 - val_loss: 0.8081 - val_time_distributed_1_loss: 0.0040 - val_dense_6_loss: 1.0091\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.38759\n",
      "('lr:', 0.0008264462809917356)\n",
      "Epoch 20/100\n",
      "39633/39633 [==============================] - 10s 262us/step - loss: 0.1671 - time_distributed_1_loss: 0.0039 - dense_6_loss: 0.2079 - val_loss: 0.8883 - val_time_distributed_1_loss: 0.0039 - val_dense_6_loss: 1.1094\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.38759\n",
      "('lr:', 0.000819672131147541)\n",
      "Epoch 21/100\n",
      "39633/39633 [==============================] - 10s 258us/step - loss: 0.1595 - time_distributed_1_loss: 0.0038 - dense_6_loss: 0.1985 - val_loss: 0.8003 - val_time_distributed_1_loss: 0.0039 - val_dense_6_loss: 0.9994\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.38759\n",
      "('lr:', 0.0008130081300813008)\n",
      "Epoch 22/100\n",
      "39633/39633 [==============================] - 10s 257us/step - loss: 0.1502 - time_distributed_1_loss: 0.0038 - dense_6_loss: 0.1868 - val_loss: 0.8644 - val_time_distributed_1_loss: 0.0039 - val_dense_6_loss: 1.0795\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.38759\n",
      "('lr:', 0.0008064516129032258)\n",
      "Epoch 23/100\n",
      "39633/39633 [==============================] - 10s 255us/step - loss: 0.1418 - time_distributed_1_loss: 0.0038 - dense_6_loss: 0.1763 - val_loss: 0.9087 - val_time_distributed_1_loss: 0.0039 - val_dense_6_loss: 1.1349\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.38759\n",
      "('lr:', 0.0008)\n",
      "Epoch 24/100\n",
      "39633/39633 [==============================] - 10s 257us/step - loss: 0.1309 - time_distributed_1_loss: 0.0038 - dense_6_loss: 0.1627 - val_loss: 0.8858 - val_time_distributed_1_loss: 0.0039 - val_dense_6_loss: 1.1063\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.38759\n",
      "('lr:', 0.0007936507936507937)\n",
      "Epoch 25/100\n",
      "39633/39633 [==============================] - 10s 255us/step - loss: 0.1273 - time_distributed_1_loss: 0.0038 - dense_6_loss: 0.1581 - val_loss: 1.0463 - val_time_distributed_1_loss: 0.0039 - val_dense_6_loss: 1.3069\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00025: val_loss did not improve from 0.38759\n",
      "('lr:', 0.0007874015748031496)\n",
      "Epoch 26/100\n",
      "39633/39633 [==============================] - 12s 299us/step - loss: 0.1237 - time_distributed_1_loss: 0.0037 - dense_6_loss: 0.1537 - val_loss: 1.0765 - val_time_distributed_1_loss: 0.0041 - val_dense_6_loss: 1.3446\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.38759\n",
      "('lr:', 0.00078125)\n",
      "Epoch 27/100\n",
      "39633/39633 [==============================] - 10s 255us/step - loss: 0.1157 - time_distributed_1_loss: 0.0039 - dense_6_loss: 0.1437 - val_loss: 0.9934 - val_time_distributed_1_loss: 0.0039 - val_dense_6_loss: 1.2407\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.38759\n",
      "('lr:', 0.0007751937984496124)\n",
      "Epoch 28/100\n",
      "39633/39633 [==============================] - 10s 255us/step - loss: 0.1069 - time_distributed_1_loss: 0.0038 - dense_6_loss: 0.1327 - val_loss: 1.0385 - val_time_distributed_1_loss: 0.0039 - val_dense_6_loss: 1.2971\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.38759\n",
      "('lr:', 0.0007692307692307692)\n",
      "Epoch 29/100\n",
      "39633/39633 [==============================] - 10s 258us/step - loss: 0.1032 - time_distributed_1_loss: 0.0037 - dense_6_loss: 0.1281 - val_loss: 1.1448 - val_time_distributed_1_loss: 0.0038 - val_dense_6_loss: 1.4300\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.38759\n",
      "('lr:', 0.0007633587786259542)\n",
      "Epoch 30/100\n",
      "39633/39633 [==============================] - 10s 258us/step - loss: 0.0998 - time_distributed_1_loss: 0.0037 - dense_6_loss: 0.1239 - val_loss: 1.0694 - val_time_distributed_1_loss: 0.0038 - val_dense_6_loss: 1.3357\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.38759\n",
      "('lr:', 0.0007575757575757576)\n",
      "Epoch 31/100\n",
      "39633/39633 [==============================] - 10s 253us/step - loss: 0.0943 - time_distributed_1_loss: 0.0036 - dense_6_loss: 0.1170 - val_loss: 1.2205 - val_time_distributed_1_loss: 0.0039 - val_dense_6_loss: 1.5246\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.38759\n",
      "('lr:', 0.0007518796992481202)\n",
      "Epoch 32/100\n",
      "39633/39633 [==============================] - 10s 251us/step - loss: 0.0937 - time_distributed_1_loss: 0.0036 - dense_6_loss: 0.1162 - val_loss: 1.2260 - val_time_distributed_1_loss: 0.0037 - val_dense_6_loss: 1.5316\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.38759\n",
      "('lr:', 0.0007462686567164179)\n",
      "Epoch 33/100\n",
      "39633/39633 [==============================] - 10s 257us/step - loss: 0.0866 - time_distributed_1_loss: 0.0036 - dense_6_loss: 0.1073 - val_loss: 1.2203 - val_time_distributed_1_loss: 0.0038 - val_dense_6_loss: 1.5245\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.38759\n",
      "('lr:', 0.0007407407407407407)\n",
      "Epoch 34/100\n",
      "39633/39633 [==============================] - 10s 258us/step - loss: 0.0832 - time_distributed_1_loss: 0.0037 - dense_6_loss: 0.1030 - val_loss: 1.2557 - val_time_distributed_1_loss: 0.0038 - val_dense_6_loss: 1.5686\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.38759\n",
      "('lr:', 0.0007352941176470589)\n",
      "Epoch 35/100\n",
      "39633/39633 [==============================] - 10s 257us/step - loss: 0.0797 - time_distributed_1_loss: 0.0038 - dense_6_loss: 0.0986 - val_loss: 1.2064 - val_time_distributed_1_loss: 0.0039 - val_dense_6_loss: 1.5070\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.38759\n",
      "('lr:', 0.00072992700729927)\n",
      "Epoch 36/100\n",
      "39633/39633 [==============================] - 10s 252us/step - loss: 0.0762 - time_distributed_1_loss: 0.0037 - dense_6_loss: 0.0943 - val_loss: 1.2440 - val_time_distributed_1_loss: 0.0039 - val_dense_6_loss: 1.5541\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.38759\n",
      "('lr:', 0.0007246376811594204)\n",
      "Epoch 37/100\n",
      "39633/39633 [==============================] - 10s 254us/step - loss: 0.0704 - time_distributed_1_loss: 0.0037 - dense_6_loss: 0.0871 - val_loss: 1.3382 - val_time_distributed_1_loss: 0.0039 - val_dense_6_loss: 1.6718\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.38759\n",
      "('lr:', 0.0007194244604316546)\n",
      "Epoch 38/100\n",
      "39633/39633 [==============================] - 10s 254us/step - loss: 0.0702 - time_distributed_1_loss: 0.0036 - dense_6_loss: 0.0868 - val_loss: 1.3026 - val_time_distributed_1_loss: 0.0038 - val_dense_6_loss: 1.6273\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.38759\n",
      "('lr:', 0.0007142857142857144)\n",
      "Epoch 39/100\n",
      "39633/39633 [==============================] - 10s 254us/step - loss: 0.0650 - time_distributed_1_loss: 0.0038 - dense_6_loss: 0.0803 - val_loss: 1.3503 - val_time_distributed_1_loss: 0.0039 - val_dense_6_loss: 1.6869\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.38759\n",
      "('lr:', 0.0007092198581560283)\n",
      "Epoch 40/100\n",
      "39633/39633 [==============================] - 10s 255us/step - loss: 0.0605 - time_distributed_1_loss: 0.0037 - dense_6_loss: 0.0747 - val_loss: 1.4319 - val_time_distributed_1_loss: 0.0039 - val_dense_6_loss: 1.7890\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.38759\n",
      "('lr:', 0.0007042253521126761)\n",
      "Epoch 41/100\n",
      "39633/39633 [==============================] - 10s 252us/step - loss: 0.0585 - time_distributed_1_loss: 0.0038 - dense_6_loss: 0.0722 - val_loss: 1.4531 - val_time_distributed_1_loss: 0.0039 - val_dense_6_loss: 1.8153\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.38759\n",
      "('lr:', 0.0006993006993006993)\n",
      "Epoch 42/100\n",
      "39633/39633 [==============================] - 10s 254us/step - loss: 0.0575 - time_distributed_1_loss: 0.0037 - dense_6_loss: 0.0710 - val_loss: 1.3960 - val_time_distributed_1_loss: 0.0040 - val_dense_6_loss: 1.7440\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.38759\n",
      "('lr:', 0.0006944444444444445)\n",
      "Epoch 43/100\n",
      "39633/39633 [==============================] - 10s 256us/step - loss: 0.0593 - time_distributed_1_loss: 0.0037 - dense_6_loss: 0.0732 - val_loss: 1.4244 - val_time_distributed_1_loss: 0.0039 - val_dense_6_loss: 1.7795\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.38759\n",
      "('lr:', 0.0006896551724137932)\n",
      "Epoch 44/100\n",
      "39633/39633 [==============================] - 10s 256us/step - loss: 0.0539 - time_distributed_1_loss: 0.0037 - dense_6_loss: 0.0665 - val_loss: 1.4576 - val_time_distributed_1_loss: 0.0038 - val_dense_6_loss: 1.8211\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.38759\n",
      "('lr:', 0.0006849315068493151)\n",
      "Epoch 45/100\n",
      "39633/39633 [==============================] - 10s 257us/step - loss: 0.0544 - time_distributed_1_loss: 0.0037 - dense_6_loss: 0.0671 - val_loss: 1.4303 - val_time_distributed_1_loss: 0.0039 - val_dense_6_loss: 1.7869\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.38759\n",
      "('lr:', 0.0006802721088435375)\n",
      "Epoch 46/100\n",
      "39633/39633 [==============================] - 10s 261us/step - loss: 0.0490 - time_distributed_1_loss: 0.0037 - dense_6_loss: 0.0603 - val_loss: 1.4117 - val_time_distributed_1_loss: 0.0037 - val_dense_6_loss: 1.7638\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.38759\n",
      "('lr:', 0.0006756756756756757)\n",
      "Epoch 47/100\n",
      "39633/39633 [==============================] - 10s 257us/step - loss: 0.0501 - time_distributed_1_loss: 0.0037 - dense_6_loss: 0.0617 - val_loss: 1.4314 - val_time_distributed_1_loss: 0.0039 - val_dense_6_loss: 1.7883\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.38759\n",
      "('lr:', 0.0006711409395973154)\n",
      "Epoch 48/100\n",
      "39633/39633 [==============================] - 10s 254us/step - loss: 0.0490 - time_distributed_1_loss: 0.0037 - dense_6_loss: 0.0603 - val_loss: 1.3997 - val_time_distributed_1_loss: 0.0040 - val_dense_6_loss: 1.7486\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.38759\n",
      "('lr:', 0.0006666666666666666)\n",
      "Epoch 49/100\n",
      "39633/39633 [==============================] - 10s 255us/step - loss: 0.0462 - time_distributed_1_loss: 0.0037 - dense_6_loss: 0.0568 - val_loss: 1.5450 - val_time_distributed_1_loss: 0.0039 - val_dense_6_loss: 1.9303\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.38759\n",
      "('lr:', 0.0006622516556291391)\n",
      "Epoch 50/100\n",
      "39633/39633 [==============================] - 10s 262us/step - loss: 0.0435 - time_distributed_1_loss: 0.0037 - dense_6_loss: 0.0534 - val_loss: 1.5375 - val_time_distributed_1_loss: 0.0038 - val_dense_6_loss: 1.9209\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.38759\n",
      "('lr:', 0.0006578947368421052)\n",
      "Epoch 51/100\n",
      "39633/39633 [==============================] - 10s 253us/step - loss: 0.0470 - time_distributed_1_loss: 0.0036 - dense_6_loss: 0.0578 - val_loss: 1.5432 - val_time_distributed_1_loss: 0.0038 - val_dense_6_loss: 1.9280\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.38759\n",
      "('lr:', 0.00065359477124183)\n",
      "Epoch 52/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39633/39633 [==============================] - 10s 260us/step - loss: 0.0383 - time_distributed_1_loss: 0.0037 - dense_6_loss: 0.0469 - val_loss: 1.5470 - val_time_distributed_1_loss: 0.0039 - val_dense_6_loss: 1.9328\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.38759\n",
      "('lr:', 0.0006493506493506494)\n",
      "Epoch 53/100\n",
      "39633/39633 [==============================] - 10s 260us/step - loss: 0.0365 - time_distributed_1_loss: 0.0036 - dense_6_loss: 0.0447 - val_loss: 1.6867 - val_time_distributed_1_loss: 0.0039 - val_dense_6_loss: 2.1074\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.38759\n",
      "('lr:', 0.0006451612903225806)\n",
      "Epoch 54/100\n",
      "39633/39633 [==============================] - 10s 258us/step - loss: 0.0350 - time_distributed_1_loss: 0.0036 - dense_6_loss: 0.0429 - val_loss: 1.6699 - val_time_distributed_1_loss: 0.0039 - val_dense_6_loss: 2.0864\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.38759\n",
      "('lr:', 0.000641025641025641)\n",
      "Epoch 55/100\n",
      "39633/39633 [==============================] - 10s 250us/step - loss: 0.0404 - time_distributed_1_loss: 0.0037 - dense_6_loss: 0.0496 - val_loss: 1.5864 - val_time_distributed_1_loss: 0.0038 - val_dense_6_loss: 1.9820\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.38759\n",
      "('lr:', 0.0006369426751592356)\n",
      "Epoch 56/100\n",
      "39633/39633 [==============================] - 11s 277us/step - loss: 0.0384 - time_distributed_1_loss: 0.0036 - dense_6_loss: 0.0471 - val_loss: 1.6970 - val_time_distributed_1_loss: 0.0038 - val_dense_6_loss: 2.1203\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.38759\n",
      "('lr:', 0.0006329113924050633)\n",
      "Epoch 57/100\n",
      "39633/39633 [==============================] - 11s 277us/step - loss: 0.0331 - time_distributed_1_loss: 0.0035 - dense_6_loss: 0.0406 - val_loss: 1.6743 - val_time_distributed_1_loss: 0.0038 - val_dense_6_loss: 2.0919\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.38759\n",
      "('lr:', 0.000628930817610063)\n",
      "Epoch 58/100\n",
      "39633/39633 [==============================] - 10s 259us/step - loss: 0.0347 - time_distributed_1_loss: 0.0036 - dense_6_loss: 0.0425 - val_loss: 1.6776 - val_time_distributed_1_loss: 0.0039 - val_dense_6_loss: 2.0960\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.38759\n",
      "('lr:', 0.000625)\n",
      "Epoch 59/100\n",
      "39633/39633 [==============================] - 10s 258us/step - loss: 0.0329 - time_distributed_1_loss: 0.0035 - dense_6_loss: 0.0403 - val_loss: 1.5710 - val_time_distributed_1_loss: 0.0038 - val_dense_6_loss: 1.9628\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.38759\n",
      "('lr:', 0.0006211180124223603)\n",
      "Epoch 60/100\n",
      "39633/39633 [==============================] - 10s 255us/step - loss: 0.0367 - time_distributed_1_loss: 0.0036 - dense_6_loss: 0.0449 - val_loss: 1.5710 - val_time_distributed_1_loss: 0.0040 - val_dense_6_loss: 1.9628\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.38759\n",
      "('lr:', 0.0006172839506172839)\n",
      "Epoch 61/100\n",
      "39633/39633 [==============================] - 10s 255us/step - loss: 0.0329 - time_distributed_1_loss: 0.0036 - dense_6_loss: 0.0402 - val_loss: 1.6500 - val_time_distributed_1_loss: 0.0039 - val_dense_6_loss: 2.0615\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.38759\n",
      "('lr:', 0.0006134969325153375)\n",
      "Epoch 62/100\n",
      "39633/39633 [==============================] - 10s 259us/step - loss: 0.0308 - time_distributed_1_loss: 0.0035 - dense_6_loss: 0.0376 - val_loss: 1.5647 - val_time_distributed_1_loss: 0.0040 - val_dense_6_loss: 1.9548\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.38759\n",
      "('lr:', 0.0006097560975609756)\n",
      "Epoch 63/100\n",
      "39633/39633 [==============================] - 10s 260us/step - loss: 0.0315 - time_distributed_1_loss: 0.0036 - dense_6_loss: 0.0385 - val_loss: 1.6120 - val_time_distributed_1_loss: 0.0038 - val_dense_6_loss: 2.0141\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.38759\n",
      "('lr:', 0.0006060606060606061)\n",
      "Epoch 64/100\n",
      "39633/39633 [==============================] - 10s 259us/step - loss: 0.0276 - time_distributed_1_loss: 0.0035 - dense_6_loss: 0.0336 - val_loss: 1.6120 - val_time_distributed_1_loss: 0.0039 - val_dense_6_loss: 2.0140\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.38759\n",
      "('lr:', 0.0006024096385542168)\n",
      "Epoch 65/100\n",
      "39633/39633 [==============================] - 10s 254us/step - loss: 0.0276 - time_distributed_1_loss: 0.0035 - dense_6_loss: 0.0336 - val_loss: 1.6629 - val_time_distributed_1_loss: 0.0038 - val_dense_6_loss: 2.0777\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.38759\n",
      "('lr:', 0.0005988023952095808)\n",
      "Epoch 66/100\n",
      "39633/39633 [==============================] - 10s 256us/step - loss: 0.0275 - time_distributed_1_loss: 0.0034 - dense_6_loss: 0.0336 - val_loss: 1.7462 - val_time_distributed_1_loss: 0.0038 - val_dense_6_loss: 2.1818\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.38759\n",
      "('lr:', 0.0005952380952380952)\n",
      "Epoch 67/100\n",
      "39633/39633 [==============================] - 10s 255us/step - loss: 0.0258 - time_distributed_1_loss: 0.0034 - dense_6_loss: 0.0313 - val_loss: 1.7660 - val_time_distributed_1_loss: 0.0038 - val_dense_6_loss: 2.2066\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.38759\n",
      "('lr:', 0.000591715976331361)\n",
      "Epoch 68/100\n",
      "39633/39633 [==============================] - 10s 259us/step - loss: 0.0248 - time_distributed_1_loss: 0.0034 - dense_6_loss: 0.0301 - val_loss: 1.6741 - val_time_distributed_1_loss: 0.0038 - val_dense_6_loss: 2.0916\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.38759\n",
      "('lr:', 0.000588235294117647)\n",
      "Epoch 69/100\n",
      "39633/39633 [==============================] - 10s 260us/step - loss: 0.0261 - time_distributed_1_loss: 0.0035 - dense_6_loss: 0.0318 - val_loss: 1.7576 - val_time_distributed_1_loss: 0.0037 - val_dense_6_loss: 2.1961\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.38759\n",
      "('lr:', 0.0005847953216374269)\n",
      "Epoch 70/100\n",
      "39633/39633 [==============================] - 10s 255us/step - loss: 0.0266 - time_distributed_1_loss: 0.0035 - dense_6_loss: 0.0324 - val_loss: 1.7695 - val_time_distributed_1_loss: 0.0038 - val_dense_6_loss: 2.2109\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.38759\n",
      "('lr:', 0.0005813953488372093)\n",
      "Epoch 71/100\n",
      "39633/39633 [==============================] - 10s 256us/step - loss: 0.0225 - time_distributed_1_loss: 0.0035 - dense_6_loss: 0.0273 - val_loss: 1.6644 - val_time_distributed_1_loss: 0.0038 - val_dense_6_loss: 2.0795\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.38759\n",
      "('lr:', 0.0005780346820809249)\n",
      "Epoch 72/100\n",
      "39633/39633 [==============================] - 10s 257us/step - loss: 0.0266 - time_distributed_1_loss: 0.0035 - dense_6_loss: 0.0323 - val_loss: 1.6397 - val_time_distributed_1_loss: 0.0039 - val_dense_6_loss: 2.0487\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.38759\n",
      "('lr:', 0.0005747126436781609)\n",
      "Epoch 73/100\n",
      "39633/39633 [==============================] - 10s 258us/step - loss: 0.0241 - time_distributed_1_loss: 0.0035 - dense_6_loss: 0.0293 - val_loss: 1.7928 - val_time_distributed_1_loss: 0.0038 - val_dense_6_loss: 2.2400\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.38759\n",
      "('lr:', 0.0005714285714285715)\n",
      "Epoch 74/100\n",
      "39633/39633 [==============================] - 10s 261us/step - loss: 0.0231 - time_distributed_1_loss: 0.0035 - dense_6_loss: 0.0279 - val_loss: 1.7414 - val_time_distributed_1_loss: 0.0039 - val_dense_6_loss: 2.1758\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.38759\n",
      "('lr:', 0.0005681818181818182)\n",
      "Epoch 75/100\n",
      "39633/39633 [==============================] - 10s 259us/step - loss: 0.0223 - time_distributed_1_loss: 0.0035 - dense_6_loss: 0.0270 - val_loss: 1.7231 - val_time_distributed_1_loss: 0.0038 - val_dense_6_loss: 2.1530\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.38759\n",
      "('lr:', 0.0005649717514124294)\n",
      "Epoch 76/100\n",
      "39633/39633 [==============================] - 10s 264us/step - loss: 0.0201 - time_distributed_1_loss: 0.0035 - dense_6_loss: 0.0243 - val_loss: 1.7989 - val_time_distributed_1_loss: 0.0039 - val_dense_6_loss: 2.2477\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.38759\n",
      "('lr:', 0.0005617977528089888)\n",
      "Epoch 77/100\n",
      "39633/39633 [==============================] - 10s 259us/step - loss: 0.0216 - time_distributed_1_loss: 0.0034 - dense_6_loss: 0.0261 - val_loss: 1.7686 - val_time_distributed_1_loss: 0.0038 - val_dense_6_loss: 2.2098\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.38759\n",
      "('lr:', 0.0005586592178770949)\n",
      "Epoch 78/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39633/39633 [==============================] - 10s 256us/step - loss: 0.0197 - time_distributed_1_loss: 0.0034 - dense_6_loss: 0.0238 - val_loss: 1.7770 - val_time_distributed_1_loss: 0.0038 - val_dense_6_loss: 2.2203\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.38759\n",
      "('lr:', 0.0005555555555555556)\n",
      "Epoch 79/100\n",
      "39633/39633 [==============================] - 10s 261us/step - loss: 0.0195 - time_distributed_1_loss: 0.0034 - dense_6_loss: 0.0235 - val_loss: 1.7415 - val_time_distributed_1_loss: 0.0037 - val_dense_6_loss: 2.1759\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.38759\n",
      "('lr:', 0.0005524861878453039)\n",
      "Epoch 80/100\n",
      "39633/39633 [==============================] - 10s 257us/step - loss: 0.0199 - time_distributed_1_loss: 0.0033 - dense_6_loss: 0.0241 - val_loss: 1.8233 - val_time_distributed_1_loss: 0.0037 - val_dense_6_loss: 2.2782\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.38759\n",
      "('lr:', 0.0005494505494505495)\n",
      "Epoch 81/100\n",
      "39633/39633 [==============================] - 10s 252us/step - loss: 0.0219 - time_distributed_1_loss: 0.0033 - dense_6_loss: 0.0266 - val_loss: 1.6717 - val_time_distributed_1_loss: 0.0037 - val_dense_6_loss: 2.0887\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.38759\n",
      "('lr:', 0.000546448087431694)\n",
      "Epoch 82/100\n",
      "39633/39633 [==============================] - 10s 252us/step - loss: 0.0191 - time_distributed_1_loss: 0.0033 - dense_6_loss: 0.0231 - val_loss: 1.7329 - val_time_distributed_1_loss: 0.0036 - val_dense_6_loss: 2.1652\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.38759\n",
      "('lr:', 0.0005434782608695652)\n",
      "Epoch 83/100\n",
      "39633/39633 [==============================] - 10s 259us/step - loss: 0.0231 - time_distributed_1_loss: 0.0032 - dense_6_loss: 0.0280 - val_loss: 1.7346 - val_time_distributed_1_loss: 0.0037 - val_dense_6_loss: 2.1674\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.38759\n",
      "('lr:', 0.0005405405405405405)\n",
      "Epoch 84/100\n",
      "39633/39633 [==============================] - 10s 260us/step - loss: 0.0243 - time_distributed_1_loss: 0.0034 - dense_6_loss: 0.0295 - val_loss: 1.7292 - val_time_distributed_1_loss: 0.0038 - val_dense_6_loss: 2.1606\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.38759\n",
      "('lr:', 0.0005376344086021505)\n",
      "Epoch 85/100\n",
      "39633/39633 [==============================] - 10s 258us/step - loss: 0.0174 - time_distributed_1_loss: 0.0034 - dense_6_loss: 0.0209 - val_loss: 1.7305 - val_time_distributed_1_loss: 0.0038 - val_dense_6_loss: 2.1622\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.38759\n",
      "('lr:', 0.0005347593582887701)\n",
      "Epoch 86/100\n",
      "39633/39633 [==============================] - 11s 266us/step - loss: 0.0171 - time_distributed_1_loss: 0.0033 - dense_6_loss: 0.0205 - val_loss: 1.6269 - val_time_distributed_1_loss: 0.0036 - val_dense_6_loss: 2.0327\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.38759\n",
      "('lr:', 0.0005319148936170213)\n",
      "Epoch 87/100\n",
      "39633/39633 [==============================] - 11s 282us/step - loss: 0.0160 - time_distributed_1_loss: 0.0033 - dense_6_loss: 0.0192 - val_loss: 1.7784 - val_time_distributed_1_loss: 0.0038 - val_dense_6_loss: 2.2221\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.38759\n",
      "('lr:', 0.000529100529100529)\n",
      "Epoch 88/100\n",
      "39633/39633 [==============================] - 10s 257us/step - loss: 0.0201 - time_distributed_1_loss: 0.0033 - dense_6_loss: 0.0243 - val_loss: 1.6688 - val_time_distributed_1_loss: 0.0038 - val_dense_6_loss: 2.0851\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.38759\n",
      "('lr:', 0.0005263157894736842)\n",
      "Epoch 89/100\n",
      "39633/39633 [==============================] - 10s 254us/step - loss: 0.0186 - time_distributed_1_loss: 0.0034 - dense_6_loss: 0.0224 - val_loss: 1.6633 - val_time_distributed_1_loss: 0.0038 - val_dense_6_loss: 2.0782\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.38759\n",
      "('lr:', 0.0005235602094240837)\n",
      "Epoch 90/100\n",
      "39633/39633 [==============================] - 10s 257us/step - loss: 0.0195 - time_distributed_1_loss: 0.0034 - dense_6_loss: 0.0235 - val_loss: 1.7704 - val_time_distributed_1_loss: 0.0038 - val_dense_6_loss: 2.2120\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.38759\n",
      "('lr:', 0.0005208333333333333)\n",
      "Epoch 91/100\n",
      "39633/39633 [==============================] - 10s 262us/step - loss: 0.0152 - time_distributed_1_loss: 0.0034 - dense_6_loss: 0.0182 - val_loss: 1.7973 - val_time_distributed_1_loss: 0.0038 - val_dense_6_loss: 2.2457\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.38759\n",
      "('lr:', 0.0005181347150259067)\n",
      "Epoch 92/100\n",
      "39633/39633 [==============================] - 10s 261us/step - loss: 0.0137 - time_distributed_1_loss: 0.0033 - dense_6_loss: 0.0163 - val_loss: 1.8581 - val_time_distributed_1_loss: 0.0037 - val_dense_6_loss: 2.3217\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.38759\n",
      "('lr:', 0.0005154639175257732)\n",
      "Epoch 93/100\n",
      "39633/39633 [==============================] - 10s 253us/step - loss: 0.0163 - time_distributed_1_loss: 0.0033 - dense_6_loss: 0.0195 - val_loss: 1.7308 - val_time_distributed_1_loss: 0.0038 - val_dense_6_loss: 2.1625\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.38759\n",
      "('lr:', 0.0005128205128205128)\n",
      "Epoch 94/100\n",
      "39633/39633 [==============================] - 10s 258us/step - loss: 0.0184 - time_distributed_1_loss: 0.0033 - dense_6_loss: 0.0222 - val_loss: 1.7054 - val_time_distributed_1_loss: 0.0038 - val_dense_6_loss: 2.1308\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.38759\n",
      "('lr:', 0.0005102040816326531)\n",
      "Epoch 95/100\n",
      "39633/39633 [==============================] - 10s 261us/step - loss: 0.0137 - time_distributed_1_loss: 0.0034 - dense_6_loss: 0.0163 - val_loss: 1.8198 - val_time_distributed_1_loss: 0.0038 - val_dense_6_loss: 2.2739\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.38759\n",
      "('lr:', 0.0005076142131979696)\n",
      "Epoch 96/100\n",
      "39633/39633 [==============================] - 10s 255us/step - loss: 0.0174 - time_distributed_1_loss: 0.0033 - dense_6_loss: 0.0209 - val_loss: 1.7437 - val_time_distributed_1_loss: 0.0037 - val_dense_6_loss: 2.1787\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.38759\n",
      "('lr:', 0.000505050505050505)\n",
      "Epoch 97/100\n",
      "39633/39633 [==============================] - 10s 256us/step - loss: 0.0133 - time_distributed_1_loss: 0.0033 - dense_6_loss: 0.0158 - val_loss: 1.8796 - val_time_distributed_1_loss: 0.0038 - val_dense_6_loss: 2.3485\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.38759\n",
      "('lr:', 0.0005025125628140704)\n",
      "Epoch 98/100\n",
      "39633/39633 [==============================] - 10s 260us/step - loss: 0.0152 - time_distributed_1_loss: 0.0033 - dense_6_loss: 0.0181 - val_loss: 1.8334 - val_time_distributed_1_loss: 0.0038 - val_dense_6_loss: 2.2908\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.38759\n",
      "('lr:', 0.0005)\n",
      "Epoch 99/100\n",
      "39633/39633 [==============================] - 10s 260us/step - loss: 0.0122 - time_distributed_1_loss: 0.0033 - dense_6_loss: 0.0144 - val_loss: 1.8819 - val_time_distributed_1_loss: 0.0038 - val_dense_6_loss: 2.3514\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.38759\n",
      "('lr:', 0.0004975124378109454)\n",
      "Epoch 100/100\n",
      "39633/39633 [==============================] - 10s 253us/step - loss: 0.0165 - time_distributed_1_loss: 0.0034 - dense_6_loss: 0.0198 - val_loss: 1.7783 - val_time_distributed_1_loss: 0.0038 - val_dense_6_loss: 2.2220\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.38759\n",
      "('lr:', 0.0004950495049504951)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3ea7c4df90>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Train model\n",
    "filepath = 'best_model_autoencoder_Manh_RF'\n",
    "    \n",
    "checkpoint = ModelCheckpoint(\n",
    "    filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', min_delta=0.00001, patience=100, verbose=0, mode='auto')\n",
    "\n",
    "sd=[]\n",
    "class LossHistory(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = [1,1]\n",
    "\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        sd.append(step_decay(len(self.losses)))\n",
    "        print('lr:', step_decay(len(self.losses)))\n",
    "\n",
    "def step_decay(losses):\n",
    "    if history.losses[-1]  < 0.48:\n",
    "        lrate=0.001*1/(1+0.01*len(history.losses))\n",
    "        momentum=0.9\n",
    "        decay_rate=2e-6\n",
    "    else:\n",
    "        lrate = 0.001\n",
    "    return lrate\n",
    "\n",
    "history=LossHistory()\n",
    "lrate=LearningRateScheduler(step_decay)\n",
    "\n",
    "callbacks_list = [checkpoint, early_stop, history, lrate]\n",
    "autoencoder.fit(normalized_train_features, [normalized_train_features, np.array(nr_labels)],\n",
    "                epochs=100,\n",
    "                batch_size=256,callbacks=callbacks_list,\n",
    "                shuffle=True, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Finished predicting', 0)\n",
      "('Finished predicting', 1000)\n",
      "('Finished predicting', 2000)\n",
      "('Finished predicting', 3000)\n",
      "('Finished predicting', 4000)\n",
      "('Finished predicting', 5000)\n",
      "('Finished predicting', 6000)\n",
      "('Finished predicting', 7000)\n",
      "('Finished predicting', 8000)\n",
      "('Finished predicting', 9000)\n",
      "('Finished predicting', 10000)\n",
      "('Finished predicting', 11000)\n",
      "('Finished predicting', 12000)\n",
      "('Finished predicting', 13000)\n"
     ]
    }
   ],
   "source": [
    "#Predict test data\n",
    "autoencoder.load_weights(filepath)\n",
    "#transformed_features = encoder.predict(normalized_train_features)\n",
    "\n",
    "# test autoencoder + nr classifier model with test data\n",
    "cohort_labels = []\n",
    "cohort_predictions = []\n",
    "cohort_probabilities = []\n",
    "for idx, data in enumerate(X_imputed_test):\n",
    "    num_row = data.shape[0]\n",
    "    num_col = data.shape[1]\n",
    "    x_input = []\n",
    "    labels = []\n",
    "    for i in range(num_row):\n",
    "        if i >=5:\n",
    "            d = data.loc[(i-5):i].values\n",
    "        else:\n",
    "            imputing_values = np.array(mean_features.values[0:40]).reshape(1, 40)\n",
    "            d = data.loc[0:i].values\n",
    "            \n",
    "            while(d.shape[0] < 6):\n",
    "                d = np.concatenate([imputing_values, d], axis=0)\n",
    "            \n",
    "        d = d.reshape((6, 40)).flatten()\n",
    "        l = int(y_imputed_test[idx].loc[i])\n",
    "        labels.append(l)\n",
    "        x_input.append(d)\n",
    "        \n",
    "        \n",
    "    x_input = np.array(x_input)\n",
    "    \n",
    "    nr_output = classifier.predict(scaler.transform(x_input).reshape((len(x_input), 6, 40)))\n",
    "    probabilities = nr_output[:,1]\n",
    "    predictions = []\n",
    "    for p in probabilities:\n",
    "        if p > 0.5:\n",
    "            predictions.append(1)\n",
    "        else: predictions.append(0)\n",
    "            \n",
    "    predictions = np.array(predictions)\n",
    "    \n",
    "    cohort_labels.append(labels)\n",
    "    cohort_predictions.append(predictions)\n",
    "    cohort_probabilities.append(probabilities)\n",
    "    if idx % 1000 == 0:\n",
    "        print(\"Finished predicting\", idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute utility\n",
    "def compute_auc(labels, predictions):\n",
    "    # Check inputs for errors.\n",
    "    if len(predictions) != len(labels):\n",
    "        raise Exception('Numbers of predictions and labels must be the same.')\n",
    "\n",
    "    n = len(labels)\n",
    "    for i in range(n):\n",
    "        if not labels[i] in (0, 1):\n",
    "            raise Exception('Labels must satisfy label == 0 or label == 1.')\n",
    "\n",
    "    for i in range(n):\n",
    "        if not 0 <= predictions[i] <= 1:\n",
    "            raise Exception('Predictions must satisfy 0 <= prediction <= 1.')\n",
    "\n",
    "    # Find prediction thresholds.\n",
    "    thresholds = np.unique(predictions)[::-1]\n",
    "    if thresholds[0] != 1:\n",
    "        thresholds = np.concatenate((np.array([1]), thresholds))\n",
    "\n",
    "    if thresholds[-1] != 0:\n",
    "        thresholds = np.concatenate((thresholds, np.array([0])))\n",
    "    m = len(thresholds)\n",
    "\n",
    "    # Populate contingency table across prediction thresholds.\n",
    "    tp = np.zeros(m)\n",
    "    fp = np.zeros(m)\n",
    "    fn = np.zeros(m)\n",
    "    tn = np.zeros(m)\n",
    "\n",
    "    # Find indices that sort predicted probabilities from largest to smallest.\n",
    "    idx = np.argsort(predictions)[::-1]\n",
    "\n",
    "    i = 0\n",
    "    for j in range(m):\n",
    "        # Initialize contingency table for j-th prediction threshold.\n",
    "        if j == 0:\n",
    "            tp[j] = 0\n",
    "            fp[j] = 0\n",
    "            fn[j] = np.sum(labels == 1)\n",
    "            tn[j] = np.sum(labels == 0)\n",
    "        else:\n",
    "            tp[j] = tp[j - 1]\n",
    "            fp[j] = fp[j - 1]\n",
    "            fn[j] = fn[j - 1]\n",
    "            tn[j] = tn[j - 1]\n",
    "\n",
    "        # Update contingency table for i-th largest prediction probability.\n",
    "        while i < n and predictions[idx[i]] >= thresholds[j]:\n",
    "            if labels[idx[i]]:\n",
    "                tp[j] += 1\n",
    "                fn[j] -= 1\n",
    "            else:\n",
    "                fp[j] += 1\n",
    "                tn[j] -= 1\n",
    "            i += 1\n",
    "\n",
    "    # Summarize contingency table.\n",
    "    tpr = np.zeros(m)\n",
    "    tnr = np.zeros(m)\n",
    "    ppv = np.zeros(m)\n",
    "    npv = np.zeros(m)\n",
    "\n",
    "    for j in range(m):\n",
    "        if tp[j] + fn[j]:\n",
    "            tpr[j] = tp[j] / (tp[j] + fn[j])\n",
    "        else:\n",
    "            tpr[j] = 1\n",
    "        if fp[j] + tn[j]:\n",
    "            tnr[j] = tn[j] / (fp[j] + tn[j])\n",
    "        else:\n",
    "            tnr[j] = 1\n",
    "        if tp[j] + fp[j]:\n",
    "            ppv[j] = tp[j] / (tp[j] + fp[j])\n",
    "        else:\n",
    "            ppv[j] = 1\n",
    "        if fn[j] + tn[j]:\n",
    "            npv[j] = tn[j] / (fn[j] + tn[j])\n",
    "        else:\n",
    "            npv[j] = 1\n",
    "\n",
    "    # Compute AUROC as the area under a piecewise linear function of TPR /\n",
    "    # sensitivity (x-axis) and TNR / specificity (y-axis) and AUPRC as the area\n",
    "    # under a piecewise constant of TPR / recall (x-axis) and PPV / precision\n",
    "    # (y-axis).\n",
    "    auroc = 0\n",
    "    auprc = 0\n",
    "    for j in range(m-1):\n",
    "        auroc += 0.5 * (tpr[j + 1] - tpr[j]) * (tnr[j + 1] + tnr[j])\n",
    "        auprc += (tpr[j + 1] - tpr[j]) * ppv[j + 1]\n",
    "\n",
    "    return auroc, auprc\n",
    "def compute_accuracy_f_measure(labels, predictions):\n",
    "    # Check inputs for errors.\n",
    "    if len(predictions) != len(labels):\n",
    "        raise Exception('Numbers of predictions and labels must be the same.')\n",
    "\n",
    "    n = len(labels)\n",
    "    for i in range(n):\n",
    "        if not labels[i] in (0, 1):\n",
    "            raise Exception('Labels must satisfy label == 0 or label == 1.')\n",
    "\n",
    "    for i in range(n):\n",
    "        if not predictions[i] in (0, 1):\n",
    "            raise Exception(\n",
    "                'Predictions must satisfy prediction == 0 or prediction == 1.')\n",
    "\n",
    "    # Populate contingency table.\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    tn = 0\n",
    "\n",
    "    for i in range(n):\n",
    "        if labels[i] and predictions[i]:\n",
    "            tp += 1\n",
    "        elif labels[i] and not predictions[i]:\n",
    "            fp += 1\n",
    "        elif not labels[i] and predictions[i]:\n",
    "            fn += 1\n",
    "        elif not labels[i] and not predictions[i]:\n",
    "            tn += 1\n",
    "\n",
    "    # Summarize contingency table.\n",
    "    if tp + fp + fn + tn:\n",
    "        accuracy = float(tp + tn) / float(tp + fp + fn + tn)\n",
    "    else:\n",
    "        accuracy = 1.0\n",
    "\n",
    "    if 2 * tp + fp + fn:\n",
    "        f_measure = float(2 * tp) / float(2 * tp + fp + fn)\n",
    "    else:\n",
    "        f_measure = 1.0\n",
    "\n",
    "    return accuracy, f_measure\n",
    "\n",
    "def compute_prediction_utility(labels, predictions, dt_early=-12, dt_optimal=-6, dt_late=3.0, max_u_tp=1, min_u_fn=-2, u_fp=-0.05, u_tn=0):\n",
    "    # Check inputs for errors.\n",
    "    if len(predictions) != len(labels):\n",
    "        raise Exception('Numbers of predictions and labels must be the same.')\n",
    "\n",
    "    n = len(labels)\n",
    "    for i in range(n):\n",
    "        if not labels[i] in (0, 1):\n",
    "            raise Exception('Labels must satisfy label == 0 or label == 1.')\n",
    "\n",
    "    for i in range(n):\n",
    "        if not predictions[i] in (0, 1):\n",
    "            raise Exception(\n",
    "                'Predictions must satisfy prediction == 0 or prediction == 1.')\n",
    "\n",
    "    if dt_early >= dt_optimal:\n",
    "        raise Exception(\n",
    "            'The earliest beneficial time for predictions must be before the optimal time.')\n",
    "\n",
    "    if dt_optimal >= dt_late:\n",
    "        raise Exception(\n",
    "            'The optimal time for predictions must be before the latest beneficial time.')\n",
    "\n",
    "    # Does the patient eventually have sepsis?\n",
    "    if any(labels):\n",
    "        is_septic = True\n",
    "        t_sepsis = min(i for i, label in enumerate(\n",
    "            labels) if label) - dt_optimal\n",
    "    else:\n",
    "        is_septic = False\n",
    "        t_sepsis = float('inf')\n",
    "\n",
    "    # Define slopes and intercept points for affine utility functions of the\n",
    "    # form u = m * t + b.\n",
    "    m_1 = float(max_u_tp) / float(dt_optimal - dt_early)\n",
    "    b_1 = -m_1 * dt_early\n",
    "    m_2 = float(-max_u_tp) / float(dt_late - dt_optimal)\n",
    "    b_2 = -m_2 * dt_late\n",
    "    m_3 = float(min_u_fn) / float(dt_late - dt_optimal)\n",
    "    b_3 = -m_3 * dt_optimal\n",
    "\n",
    "    # Compare predicted and true conditions.\n",
    "    u = np.zeros(n)\n",
    "    for t in range(n):\n",
    "        if t <= t_sepsis + dt_late:\n",
    "            # TP\n",
    "            if is_septic and predictions[t]:\n",
    "                if t <= t_sepsis + dt_optimal:\n",
    "                    u[t] = max(m_1 * (t - t_sepsis) + b_1, u_fp)\n",
    "                elif t <= t_sepsis + dt_late:\n",
    "                    u[t] = m_2 * (t - t_sepsis) + b_2\n",
    "            # FN\n",
    "            elif is_septic and not predictions[t]:\n",
    "                if t <= t_sepsis + dt_optimal:\n",
    "                    u[t] = 0\n",
    "                elif t <= t_sepsis + dt_late:\n",
    "                    u[t] = m_3 * (t - t_sepsis) + b_3\n",
    "            # FP\n",
    "            elif not is_septic and predictions[t]:\n",
    "                u[t] = u_fp\n",
    "            # TN\n",
    "            elif not is_septic and not predictions[t]:\n",
    "                u[t] = u_tn\n",
    "\n",
    "    # Find total utility for patient.\n",
    "    return np.sum(u)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUCROC, AUCPRC\n",
      "(0.7944489676226631, 0.08336374148205733)\n",
      "Accuaracy, F-measure\n",
      "(0.8847423624163688, 0.1330813519096383)\n",
      "('Utility, ', 0.3693794719279285)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "labels = np.concatenate(cohort_labels)\n",
    "predictions = np.concatenate(cohort_predictions)\n",
    "probabilities = np.concatenate(cohort_probabilities)\n",
    "\n",
    "auroc, auprc = compute_auc(labels, probabilities)\n",
    "accuracy, f_measure = compute_accuracy_f_measure(labels, predictions)\n",
    "\n",
    "print(\"AUCROC, AUCPRC\")\n",
    "print(auroc, auprc)\n",
    "print(\"Accuaracy, F-measure\")\n",
    "print(accuracy, f_measure) \n",
    "\n",
    "num_files = len(y_imputed_test)\n",
    "dt_early = -12\n",
    "dt_optimal = -6\n",
    "dt_late = 3\n",
    "\n",
    "max_u_tp = 1\n",
    "min_u_fn = -2\n",
    "u_fp = -0.05\n",
    "u_tn = 0\n",
    "# Compute utility.\n",
    "observed_utilities = np.zeros(num_files)\n",
    "best_utilities = np.zeros(num_files)\n",
    "worst_utilities = np.zeros(num_files)\n",
    "inaction_utilities = np.zeros(num_files)\n",
    "\n",
    "for k in range(num_files):\n",
    "    labels = cohort_labels[k]\n",
    "    num_records = len(labels)\n",
    "    observed_predictions = cohort_predictions[k]\n",
    "    best_predictions = np.zeros(num_records)\n",
    "    worst_predictions = np.zeros(num_records)\n",
    "    inaction_predictions = np.zeros(num_records)\n",
    "\n",
    "    if any(labels):\n",
    "        t_sepsis = min(i for i, label in enumerate(\n",
    "            labels) if label) - dt_optimal\n",
    "        best_predictions[max(0, t_sepsis + dt_early)\n",
    "                             : min(t_sepsis + dt_late, num_records)] = 1\n",
    "    else:\n",
    "        best_predictions[:] = 0\n",
    "    worst_predictions = 1 - best_predictions\n",
    "\n",
    "    observed_utilities[k] = compute_prediction_utility(\n",
    "        labels, observed_predictions, dt_early, dt_optimal, dt_late, max_u_tp, min_u_fn, u_fp, u_tn)\n",
    "    best_utilities[k] = compute_prediction_utility(\n",
    "        labels, best_predictions, dt_early, dt_optimal, dt_late, max_u_tp, min_u_fn, u_fp, u_tn)\n",
    "    worst_utilities[k] = compute_prediction_utility(\n",
    "        labels, worst_predictions, dt_early, dt_optimal, dt_late, max_u_tp, min_u_fn, u_fp, u_tn)\n",
    "    inaction_utilities[k] = compute_prediction_utility(\n",
    "        labels, inaction_predictions, dt_early, dt_optimal, dt_late, max_u_tp, min_u_fn, u_fp, u_tn)\n",
    "\n",
    "unnormalized_observed_utility = np.sum(observed_utilities)\n",
    "unnormalized_best_utility = np.sum(best_utilities)\n",
    "unnormalized_worst_utility = np.sum(worst_utilities)\n",
    "unnormalized_inaction_utility = np.sum(inaction_utilities)\n",
    "\n",
    "if not (unnormalized_worst_utility <= unnormalized_best_utility and unnormalized_inaction_utility <= unnormalized_best_utility):\n",
    "    raise Exception(\n",
    "        'Optimal utility must be higher than inaction utility.')\n",
    "\n",
    "normalized_observed_utility = (unnormalized_observed_utility - unnormalized_inaction_utility) / (\n",
    "    unnormalized_best_utility - unnormalized_inaction_utility)\n",
    "\n",
    "print(\"Utility, \", normalized_observed_utility)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test random forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators=500, max_depth=70,random_state=0, bootstrap=True, max_features='auto',\n",
    "                             min_samples_leaf=4, min_samples_split=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=70, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=4, min_samples_split=10,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=500, n_jobs=1,\n",
       "            oob_score=False, random_state=0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_features = normalized_train_features.reshape(-1, 240)\n",
    "count0 = 0\n",
    "count1 = 0\n",
    "train_labels = []\n",
    "for y in nr_labels:\n",
    "    if y[0] == 0:\n",
    "        train_labels.append(1)\n",
    "        count1 +=1\n",
    "    else:\n",
    "        train_labels.append(0)\n",
    "        count0 +=1\n",
    "clf.fit(rf_features, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Finished predicting', 0)\n",
      "('Finished predicting', 1000)\n",
      "('Finished predicting', 2000)\n",
      "('Finished predicting', 3000)\n",
      "('Finished predicting', 4000)\n",
      "('Finished predicting', 5000)\n",
      "('Finished predicting', 6000)\n",
      "('Finished predicting', 7000)\n",
      "('Finished predicting', 8000)\n",
      "('Finished predicting', 9000)\n",
      "('Finished predicting', 10000)\n",
      "('Finished predicting', 11000)\n",
      "('Finished predicting', 12000)\n",
      "('Finished predicting', 13000)\n"
     ]
    }
   ],
   "source": [
    "# test RF  model with test data\n",
    "cohort_labels = []\n",
    "cohort_predictions = []\n",
    "cohort_probabilities = []\n",
    "for idx, data in enumerate(X_imputed_test):\n",
    "    num_row = data.shape[0]\n",
    "    num_col = data.shape[1]\n",
    "    x_input = []\n",
    "    labels = []\n",
    "    for i in range(num_row):\n",
    "        if i >=5:\n",
    "            d = data.loc[(i-5):i].values\n",
    "        else:\n",
    "            imputing_values = np.array(mean_features.values[0:40]).reshape(1, 40)\n",
    "            d = data.loc[0:i].values\n",
    "            \n",
    "            while(d.shape[0] < 6):\n",
    "                d = np.concatenate([imputing_values, d], axis=0)\n",
    "            \n",
    "        d = d.reshape((6, 40)).flatten()\n",
    "        l = int(y_imputed_test[idx].loc[i])\n",
    "        labels.append(l)\n",
    "        x_input.append(d)\n",
    "        \n",
    "        \n",
    "    x_input = np.array(x_input)\n",
    "    \n",
    "    nr_output = clf.predict_proba(scaler.transform(x_input))\n",
    "    probabilities = nr_output[:,1]\n",
    "    predictions = []\n",
    "    for p in probabilities:\n",
    "        if p > 0.5:\n",
    "            predictions.append(1)\n",
    "        else: predictions.append(0)\n",
    "            \n",
    "    predictions = np.array(predictions)\n",
    "    \n",
    "    cohort_labels.append(labels)\n",
    "    cohort_predictions.append(predictions)\n",
    "    cohort_probabilities.append(probabilities)\n",
    "    if idx % 1000 == 0:\n",
    "        print(\"Finished predicting\", idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUCROC, AUCPRC\n",
      "(0.8170527206758389, 0.0906371017130265)\n",
      "Accuaracy, F-measure\n",
      "(0.9261591500053563, 0.16386934562537217)\n",
      "('Utility, ', 0.3472239533617853)\n"
     ]
    }
   ],
   "source": [
    "#compute utility for random forest \n",
    "labels = np.concatenate(cohort_labels)\n",
    "predictions = np.concatenate(cohort_predictions)\n",
    "probabilities = np.concatenate(cohort_probabilities)\n",
    "\n",
    "auroc, auprc = compute_auc(labels, probabilities)\n",
    "accuracy, f_measure = compute_accuracy_f_measure(labels, predictions)\n",
    "\n",
    "print(\"AUCROC, AUCPRC\")\n",
    "print(auroc, auprc)\n",
    "print(\"Accuaracy, F-measure\")\n",
    "print(accuracy, f_measure) \n",
    "\n",
    "num_files = len(y_imputed_test)\n",
    "dt_early = -12\n",
    "dt_optimal = -6\n",
    "dt_late = 3\n",
    "\n",
    "max_u_tp = 1\n",
    "min_u_fn = -2\n",
    "u_fp = -0.05\n",
    "u_tn = 0\n",
    "# Compute utility.\n",
    "observed_utilities = np.zeros(num_files)\n",
    "best_utilities = np.zeros(num_files)\n",
    "worst_utilities = np.zeros(num_files)\n",
    "inaction_utilities = np.zeros(num_files)\n",
    "\n",
    "for k in range(num_files):\n",
    "    labels = cohort_labels[k]\n",
    "    num_records = len(labels)\n",
    "    observed_predictions = cohort_predictions[k]\n",
    "    best_predictions = np.zeros(num_records)\n",
    "    worst_predictions = np.zeros(num_records)\n",
    "    inaction_predictions = np.zeros(num_records)\n",
    "\n",
    "    if any(labels):\n",
    "        t_sepsis = min(i for i, label in enumerate(\n",
    "            labels) if label) - dt_optimal\n",
    "        best_predictions[max(0, t_sepsis + dt_early)\n",
    "                             : min(t_sepsis + dt_late, num_records)] = 1\n",
    "    else:\n",
    "        best_predictions[:] = 0\n",
    "    worst_predictions = 1 - best_predictions\n",
    "\n",
    "    observed_utilities[k] = compute_prediction_utility(\n",
    "        labels, observed_predictions, dt_early, dt_optimal, dt_late, max_u_tp, min_u_fn, u_fp, u_tn)\n",
    "    best_utilities[k] = compute_prediction_utility(\n",
    "        labels, best_predictions, dt_early, dt_optimal, dt_late, max_u_tp, min_u_fn, u_fp, u_tn)\n",
    "    worst_utilities[k] = compute_prediction_utility(\n",
    "        labels, worst_predictions, dt_early, dt_optimal, dt_late, max_u_tp, min_u_fn, u_fp, u_tn)\n",
    "    inaction_utilities[k] = compute_prediction_utility(\n",
    "        labels, inaction_predictions, dt_early, dt_optimal, dt_late, max_u_tp, min_u_fn, u_fp, u_tn)\n",
    "\n",
    "unnormalized_observed_utility = np.sum(observed_utilities)\n",
    "unnormalized_best_utility = np.sum(best_utilities)\n",
    "unnormalized_worst_utility = np.sum(worst_utilities)\n",
    "unnormalized_inaction_utility = np.sum(inaction_utilities)\n",
    "\n",
    "if not (unnormalized_worst_utility <= unnormalized_best_utility and unnormalized_inaction_utility <= unnormalized_best_utility):\n",
    "    raise Exception(\n",
    "        'Optimal utility must be higher than inaction utility.')\n",
    "\n",
    "normalized_observed_utility = (unnormalized_observed_utility - unnormalized_inaction_utility) / (\n",
    "    unnormalized_best_utility - unnormalized_inaction_utility)\n",
    "\n",
    "print(\"Utility, \", normalized_observed_utility)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['best_model_rf_FT']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Save rf model\n",
    "from sklearn.externals import joblib\n",
    "rf_filepath = 'best_model_rf_FT'\n",
    "joblib.dump(clf, rf_filepath, compress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#XGBoost\n",
    "from xgboost import XGBClassifier\n",
    "xgb = XGBClassifier(objective= 'binary:logistic', colsample_bytree=0.8, learning_rate=0.1, min_child_weight=5,\n",
    "                    n_estimators=500, subsample=0.8, max_depth=5, gamma=0.1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=0.8, gamma=0.1, learning_rate=0.1,\n",
       "       max_delta_step=0, max_depth=5, min_child_weight=5, missing=None,\n",
       "       n_estimators=500, n_jobs=1, nthread=None,\n",
       "       objective='binary:logistic', random_state=0, reg_alpha=0,\n",
       "       reg_lambda=1, scale_pos_weight=1, seed=None, silent=True,\n",
       "       subsample=0.8)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb.fit(rf_features, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Finished predicting', 0)\n",
      "('Finished predicting', 1000)\n",
      "('Finished predicting', 2000)\n",
      "('Finished predicting', 3000)\n",
      "('Finished predicting', 4000)\n",
      "('Finished predicting', 5000)\n",
      "('Finished predicting', 6000)\n",
      "('Finished predicting', 7000)\n",
      "('Finished predicting', 8000)\n",
      "('Finished predicting', 9000)\n",
      "('Finished predicting', 10000)\n",
      "('Finished predicting', 11000)\n",
      "('Finished predicting', 12000)\n",
      "('Finished predicting', 13000)\n"
     ]
    }
   ],
   "source": [
    "# test XGboost  model with test data\n",
    "cohort_labels = []\n",
    "cohort_predictions = []\n",
    "cohort_probabilities = []\n",
    "for idx, data in enumerate(X_imputed_test):\n",
    "    num_row = data.shape[0]\n",
    "    num_col = data.shape[1]\n",
    "    x_input = []\n",
    "    labels = []\n",
    "    for i in range(num_row):\n",
    "        if i >=5:\n",
    "            d = data.loc[(i-5):i].values\n",
    "        else:\n",
    "            imputing_values = np.array(mean_features.values[0:40]).reshape(1, 40)\n",
    "            d = data.loc[0:i].values\n",
    "            \n",
    "            while(d.shape[0] < 6):\n",
    "                d = np.concatenate([imputing_values, d], axis=0)\n",
    "            \n",
    "        d = d.reshape((6, 40)).flatten()\n",
    "        l = int(y_imputed_test[idx].loc[i])\n",
    "        labels.append(l)\n",
    "        x_input.append(d)\n",
    "        \n",
    "        \n",
    "    x_input = np.array(x_input)\n",
    "    \n",
    "    nr_output = xgb.predict_proba(scaler.transform(x_input))\n",
    "    probabilities = nr_output[:,1]\n",
    "    predictions = []\n",
    "    for p in probabilities:\n",
    "        if p > 0.5:\n",
    "            predictions.append(1)\n",
    "        else: predictions.append(0)\n",
    "            \n",
    "    predictions = np.array(predictions)\n",
    "    \n",
    "    cohort_labels.append(labels)\n",
    "    cohort_predictions.append(predictions)\n",
    "    cohort_probabilities.append(probabilities)\n",
    "    if idx % 1000 == 0:\n",
    "        print(\"Finished predicting\", idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUCROC, AUCPRC\n",
      "(0.8192279586377739, 0.09921220055488239)\n",
      "Accuaracy, F-measure\n",
      "(0.9054936065366225, 0.15655257531246197)\n",
      "('Utility, ', 0.3983772867477102)\n"
     ]
    }
   ],
   "source": [
    "#Compute utility for xgboost \n",
    "labels = np.concatenate(cohort_labels)\n",
    "predictions = np.concatenate(cohort_predictions)\n",
    "probabilities = np.concatenate(cohort_probabilities)\n",
    "\n",
    "auroc, auprc = compute_auc(labels, probabilities)\n",
    "accuracy, f_measure = compute_accuracy_f_measure(labels, predictions)\n",
    "\n",
    "print(\"AUCROC, AUCPRC\")\n",
    "print(auroc, auprc)\n",
    "print(\"Accuaracy, F-measure\")\n",
    "print(accuracy, f_measure) \n",
    "\n",
    "num_files = len(y_imputed_test)\n",
    "dt_early = -12\n",
    "dt_optimal = -6\n",
    "dt_late = 3\n",
    "\n",
    "max_u_tp = 1\n",
    "min_u_fn = -2\n",
    "u_fp = -0.05\n",
    "u_tn = 0\n",
    "# Compute utility.\n",
    "observed_utilities = np.zeros(num_files)\n",
    "best_utilities = np.zeros(num_files)\n",
    "worst_utilities = np.zeros(num_files)\n",
    "inaction_utilities = np.zeros(num_files)\n",
    "\n",
    "for k in range(num_files):\n",
    "    labels = cohort_labels[k]\n",
    "    num_records = len(labels)\n",
    "    observed_predictions = cohort_predictions[k]\n",
    "    best_predictions = np.zeros(num_records)\n",
    "    worst_predictions = np.zeros(num_records)\n",
    "    inaction_predictions = np.zeros(num_records)\n",
    "\n",
    "    if any(labels):\n",
    "        t_sepsis = min(i for i, label in enumerate(\n",
    "            labels) if label) - dt_optimal\n",
    "        best_predictions[max(0, t_sepsis + dt_early)\n",
    "                             : min(t_sepsis + dt_late, num_records)] = 1\n",
    "    else:\n",
    "        best_predictions[:] = 0\n",
    "    worst_predictions = 1 - best_predictions\n",
    "\n",
    "    observed_utilities[k] = compute_prediction_utility(\n",
    "        labels, observed_predictions, dt_early, dt_optimal, dt_late, max_u_tp, min_u_fn, u_fp, u_tn)\n",
    "    best_utilities[k] = compute_prediction_utility(\n",
    "        labels, best_predictions, dt_early, dt_optimal, dt_late, max_u_tp, min_u_fn, u_fp, u_tn)\n",
    "    worst_utilities[k] = compute_prediction_utility(\n",
    "        labels, worst_predictions, dt_early, dt_optimal, dt_late, max_u_tp, min_u_fn, u_fp, u_tn)\n",
    "    inaction_utilities[k] = compute_prediction_utility(\n",
    "        labels, inaction_predictions, dt_early, dt_optimal, dt_late, max_u_tp, min_u_fn, u_fp, u_tn)\n",
    "\n",
    "unnormalized_observed_utility = np.sum(observed_utilities)\n",
    "unnormalized_best_utility = np.sum(best_utilities)\n",
    "unnormalized_worst_utility = np.sum(worst_utilities)\n",
    "unnormalized_inaction_utility = np.sum(inaction_utilities)\n",
    "\n",
    "if not (unnormalized_worst_utility <= unnormalized_best_utility and unnormalized_inaction_utility <= unnormalized_best_utility):\n",
    "    raise Exception(\n",
    "        'Optimal utility must be higher than inaction utility.')\n",
    "\n",
    "normalized_observed_utility = (unnormalized_observed_utility - unnormalized_inaction_utility) / (\n",
    "    unnormalized_best_utility - unnormalized_inaction_utility)\n",
    "\n",
    "print(\"Utility, \", normalized_observed_utility)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['best_model_xg_boost_FT']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Save xgboost\n",
    "from sklearn.externals import joblib\n",
    "xgb_filepath = 'best_model_xg_boost_FT'\n",
    "joblib.dump(xgb, xgb_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['scaler']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#save scaler\n",
    "scaler_file_path = 'scaler'\n",
    "joblib.dump(scaler, scaler_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mean_features']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_feature_file_path = 'mean_features'\n",
    "joblib.dump(mean_features, mean_feature_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Finished predicting', 0)\n",
      "('Finished predicting', 1000)\n",
      "('Finished predicting', 2000)\n",
      "('Finished predicting', 3000)\n",
      "('Finished predicting', 4000)\n",
      "('Finished predicting', 5000)\n",
      "('Finished predicting', 6000)\n",
      "('Finished predicting', 7000)\n",
      "('Finished predicting', 8000)\n",
      "('Finished predicting', 9000)\n",
      "('Finished predicting', 10000)\n",
      "('Finished predicting', 11000)\n",
      "('Finished predicting', 12000)\n",
      "('Finished predicting', 13000)\n"
     ]
    }
   ],
   "source": [
    "#Combine xgboost, rf, and dl\n",
    "#Predict test data\n",
    "autoencoder.load_weights(filepath)\n",
    "#transformed_features = encoder.predict(normalized_train_features)\n",
    "\n",
    "# test autoencoder + nr classifier model with test data\n",
    "cohort_labels = []\n",
    "cohort_predictions = []\n",
    "cohort_probabilities = []\n",
    "for idx, data in enumerate(X_imputed_test):\n",
    "    num_row = data.shape[0]\n",
    "    num_col = data.shape[1]\n",
    "    x_input = []\n",
    "    labels = []\n",
    "    for i in range(num_row):\n",
    "        if i >=5:\n",
    "            d = data.loc[(i-5):i].values\n",
    "        else:\n",
    "            imputing_values = np.array(mean_features.values[0:40]).reshape(1, 40)\n",
    "            d = data.loc[0:i].values\n",
    "            \n",
    "            while(d.shape[0] < 6):\n",
    "                d = np.concatenate([imputing_values, d], axis=0)\n",
    "            \n",
    "        d = d.reshape((6, 40)).flatten()\n",
    "        l = int(y_imputed_test[idx].loc[i])\n",
    "        labels.append(l)\n",
    "        x_input.append(d)\n",
    "        \n",
    "        \n",
    "    x_input = np.array(x_input)\n",
    "    \n",
    "    nr_output = classifier.predict(scaler.transform(x_input).reshape((len(x_input), 6, 40)))\n",
    "    nr_probabilities = nr_output[:,1]\n",
    "    \n",
    "    \n",
    "    xgb_output = xgb.predict_proba(scaler.transform(x_input))\n",
    "    xgb_probabilities = xgb_output[:,1]\n",
    "    \n",
    "    \n",
    "    rf_output = clf.predict_proba(scaler.transform(x_input))\n",
    "    rf_probabilities = rf_output[:,1]\n",
    "    \n",
    "    probabilities = []\n",
    "    predictions = []\n",
    "    for nr_p, xgb_p, rf_p in zip(nr_probabilities, xgb_probabilities, rf_probabilities):\n",
    "        probabilities.append(np.max([nr_p,xgb_p,rf_p]))\n",
    "        if (nr_p > 0.5 or xgb_p > 0.5 or rf_p > 0.5):\n",
    "            predictions.append(1)\n",
    "        else: predictions.append(0)\n",
    "            \n",
    "    predictions = np.array(predictions)\n",
    "    \n",
    "    cohort_labels.append(labels)\n",
    "    cohort_predictions.append(predictions)\n",
    "    cohort_probabilities.append(probabilities)\n",
    "    if idx % 1000 == 0:\n",
    "        print(\"Finished predicting\", idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUCROC, AUCPRC\n",
      "(0.8174128524208188, 0.09779186006432865)\n",
      "Accuaracy, F-measure\n",
      "(0.861254540673724, 0.12870003424825088)\n",
      "('Utility, ', 0.41137950348287744)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "labels = np.concatenate(cohort_labels)\n",
    "predictions = np.concatenate(cohort_predictions)\n",
    "probabilities = np.concatenate(cohort_probabilities)\n",
    "\n",
    "#f = open('/home/manhnh594/predictions413.txt', 'w')\n",
    "#for i in predictions:\n",
    "#    f.write('%d \\n' %i)\n",
    "#f.close()\n",
    "#print(\"Created file successfully\")\n",
    "\n",
    "auroc, auprc = compute_auc(labels, probabilities)\n",
    "accuracy, f_measure = compute_accuracy_f_measure(labels, predictions)\n",
    "\n",
    "print(\"AUCROC, AUCPRC\")\n",
    "print(auroc, auprc)\n",
    "print(\"Accuaracy, F-measure\")\n",
    "print(accuracy, f_measure) \n",
    "\n",
    "num_files = len(y_imputed_test)\n",
    "dt_early = -12\n",
    "dt_optimal = -6\n",
    "dt_late = 3\n",
    "\n",
    "max_u_tp = 1\n",
    "min_u_fn = -2\n",
    "u_fp = -0.05\n",
    "u_tn = 0\n",
    "# Compute utility.\n",
    "observed_utilities = np.zeros(num_files)\n",
    "best_utilities = np.zeros(num_files)\n",
    "worst_utilities = np.zeros(num_files)\n",
    "inaction_utilities = np.zeros(num_files)\n",
    "\n",
    "for k in range(num_files):\n",
    "    labels = cohort_labels[k]\n",
    "    num_records = len(labels)\n",
    "    observed_predictions = cohort_predictions[k]\n",
    "    best_predictions = np.zeros(num_records)\n",
    "    worst_predictions = np.zeros(num_records)\n",
    "    inaction_predictions = np.zeros(num_records)\n",
    "\n",
    "    if any(labels):\n",
    "        t_sepsis = min(i for i, label in enumerate(\n",
    "            labels) if label) - dt_optimal\n",
    "        best_predictions[max(0, t_sepsis + dt_early)\n",
    "                             : min(t_sepsis + dt_late, num_records)] = 1\n",
    "    else:\n",
    "        best_predictions[:] = 0\n",
    "    worst_predictions = 1 - best_predictions\n",
    "\n",
    "    observed_utilities[k] = compute_prediction_utility(\n",
    "        labels, observed_predictions, dt_early, dt_optimal, dt_late, max_u_tp, min_u_fn, u_fp, u_tn)\n",
    "    best_utilities[k] = compute_prediction_utility(\n",
    "        labels, best_predictions, dt_early, dt_optimal, dt_late, max_u_tp, min_u_fn, u_fp, u_tn)\n",
    "    worst_utilities[k] = compute_prediction_utility(\n",
    "        labels, worst_predictions, dt_early, dt_optimal, dt_late, max_u_tp, min_u_fn, u_fp, u_tn)\n",
    "    inaction_utilities[k] = compute_prediction_utility(\n",
    "        labels, inaction_predictions, dt_early, dt_optimal, dt_late, max_u_tp, min_u_fn, u_fp, u_tn)\n",
    "\n",
    "unnormalized_observed_utility = np.sum(observed_utilities)\n",
    "unnormalized_best_utility = np.sum(best_utilities)\n",
    "unnormalized_worst_utility = np.sum(worst_utilities)\n",
    "unnormalized_inaction_utility = np.sum(inaction_utilities)\n",
    "\n",
    "if not (unnormalized_worst_utility <= unnormalized_best_utility and unnormalized_inaction_utility <= unnormalized_best_utility):\n",
    "    raise Exception(\n",
    "        'Optimal utility must be higher than inaction utility.')\n",
    "\n",
    "normalized_observed_utility = (unnormalized_observed_utility - unnormalized_inaction_utility) / (\n",
    "    unnormalized_best_utility - unnormalized_inaction_utility)\n",
    "\n",
    "print(\"Utility, \", normalized_observed_utility)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
